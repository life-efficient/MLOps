{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Intro to Feature Stores"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature Stores are central hubs that store features, so they can be registered, discovered, and used both as part of a pipeline (ETLing for example), as well as part of inferencing. In other words, feature store allows you to transform raw data into feature values, store those values, and use them for (re)training a model or for making predictions. They also act as a single source of truth of the value of that feature, so that everyone is talking about the same thing rather than computing it in a different and potentially incorrect way. By automating these steps, feature stores allow data scientists to build and deploy features that the rest of the company can use within hours.\n",
    "\n",
    "One example is that you might have a huge dataset, and features that you engineered are becoming painfully slow to compute due to the great amount of data to be processed. For example, you might have a useful feature \"Average purchase value\" which, if you have millions of purchases, could take a while to compute this value. Each time you compute a feature you have to process all data; an alternative would be storing a running feature, so next time you add more data, you can access your feature store and grab the feature without processing the whole dataset.\n",
    "\n",
    "<p><img src=images/Feature_Stores_API.png></p>\n",
    "\n",
    "> ## Feature Stores are the interface between data and models\n",
    "\n",
    "Feature stores make it easy to:\n",
    "<details>\n",
    "  <summary> \n",
    "    <b>1. Use features without thinking the intrincacies of engineering them</b>\n",
    "  </summary>\n",
    "\n",
    "  Most of the features depend on a couple of operations because those are not very specific to the field. However, when working in a company, the features you would need might depend on a team of engineers that has a deep knowledge on the field, and they will create huge formulas or equations. <br><br>\n",
    "  \n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary> \n",
    "    <b> 2. Automate feature computation, backfills, and logging</b>\n",
    "  </summary>\n",
    "\n",
    "  As mentioned in point 1, you might need to use features created by another team. Once those features are stored, next time you add data, the features will be automatically computed for you, and you will keep track of what features correspond to what data. Also, if you missed previous data points, features stores can fill those missing points (backfills).<br><br>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary> \n",
    "    <b> 3. Share and reuse feature pipelines across teams</b>\n",
    "  </summary>\n",
    "\n",
    "  Features created by the engineering team can be used by the data scientist team as mentioned in point 1. But the other way around is also possible. The features you create can be useful for the engineering team to check if the pipeline they are developing will be useful for the whole team.<br><br>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary> \n",
    "    <b> 4. Achieve consistency between training and serving data</b>\n",
    "  </summary>\n",
    "\n",
    "  Right now, this doesn't seem very important, since most (if not all) of the features you use are used by both training and inference. However, in large projects, coordinating the data scientist and the data analysis teams might not be the easiest of the tasks. Features have to be consistent to make sure the predictions and the trained model are based on the same data. <br><br>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary> \n",
    "    <b> 5. Monitor the health of feature pipelines in production</b>\n",
    "  </summary>\n",
    "\n",
    "  After uploading the feature pipeline, you will be able to check if the used pipeline is providing good results. You can track the features and the metrics they generated, so feature stores make easier observing which feature works and which doesn't.<br><br>\n",
    "</details>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Features - Offline or Online\n",
    "\n",
    "You know by now that a feature is an input variable. However, what is the difference between _Offline_ and _Online_ features?\n",
    "\n",
    "- __Offline features__: These are static, historic features that don’t change much, and are processed in batch. Usually, offline features are calculated via frameworks such as Spark or by simply running SQL queries against a given database and then using a batch inference process. These features can be properties like patient age, a transaction location, or an IP address.<br><br>\n",
    "\n",
    "- __Online features__: These features are dynamic and require a processing engine to calculate, sometimes in near-real time. They often need to be served in ultra-low latency. For example, calculating a z-score for real-time fraud detection. In this case, the pipeline is built by calculating the mean and the standard deviation over a sliding window in real time. These calculations are much more challenging, requiring fast computation as well as fast access to the data. The data can be stored in memory or in a very fast key-value database. The process itself can be performed on various services in the cloud or on an MLOps platform.\n",
    "\n",
    "An even more complicated type of feature is engineered by using an ML process to generate the feature from a datapoint. An example of this might be to create a “contains a positive product review” feature by using NLP to identify reviews discussing a product in a positive manner."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Store Components\n",
    "\n",
    "There are 5 main components of a modern feature store: Transformation, Storage, Serving, Monitoring, and feature Registry.\n",
    "\n",
    "## Serving\n",
    "\n",
    "Feature stores serve feature data to models. Those models require a consistent view of features across training and serving. The definitions of features used to train a model must exactly match the features provided in online serving. When they don’t match, training-serving skew is introduced which can cause catastrophic and hard-to-debug model performance problems. This can be caused by different teams calculating features in slightly different ways, or forgetting to normalise data amongst other reasons.\n",
    "\n",
    "Feature stores abstract away the logic and processing used to generate a feature, providing users an easy and canonical way to access all features in a company consistently across all environments in which they’re needed.\n",
    "\n",
    "When retrieving data offline (i.e. for training), feature values are commonly accessed through notebook-friendly feature store SDKs. They provide point-in-time correct views of the state of the world for each example used to train a model (a.k.a. “time-travel”).\n",
    "For online serving, a feature store delivers a single vector of features at a time made up of the freshest feature values. Responses are served through a high-performance API backed by a low-latency database."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Storage\r\n",
    "\r\n",
    "Feature stores persist feature data to support retrieval through feature serving layers. They typically contain both an online and offline storage layer to support the requirements of different feature serving systems.\r\n",
    "\r\n",
    "Offline storage layers are typically used to store months’ or years’ worth of feature data for training purposes. Offline feature store data is often stored in data warehouses or data lakes like S3, BigQuery, Snowflake, Redshift. Extending an existing data lake or data warehouse for offline feature storage is typically preferred to prevent data silos.\r\n",
    "\r\n",
    "Online storage layers are used to persist feature values for low-latency lookup during inference. They typically only store the latest feature values for each entity, essentially modeling the current state of the world. Online stores are usually eventually consistent, and do not have strict consistency requirements for most ML use cases. They are usually implemented with key-value stores like DynamoDB, Redis, or Cassandra.\r\n",
    "\r\n",
    "<p align=center><img src=images/Feature_Store_Storage.png width=500></p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature stores use an entity-based data model where each feature value is associated with an entity (e.g. a user) and a timestamp. An entity-based data model provides minimal structure to support standardized feature management, fits naturally with common feature engineering workflows, and allows for simple retrieval queries in production."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Operational ML applications require regular processing of new data into feature values so models can make predictions using an up-to-date view of the world. Feature stores both manage and orchestrate data transformations that produce these values, as well as ingest values produced by external systems. Transformations managed by feature stores are configured by definitions in a common feature registry."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Most teams getting started with feature stores already have existing data pipelines producing feature values. This makes it very important for feature stores to be gradually adoptable and have first class integrations with existing data platforms, allowing teams to immediately operationalize existing ETL pipelines for their ML use cases._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Feature Type \t| Definition \t| Example \t|\r\n",
    "|---\t|---\t|---\t|\r\n",
    "| Batch Transform \t| Transformations that are applied only to data at rest \t| User country, product category \t|\r\n",
    "| Streaming Transform \t| Transformations that are applied to streaming sources \t| Number of clicks per vertical per user in last 30 minutes, Number of views per listing in past hour \t|\r\n",
    "| On-demand transform \t| Transformations  that are used to produce features  based on data that is only available  at the time of the prediction. These features cannot be pre-computed. \t| Is the user currently in a supported location? Similarity score between listing and search query \t|"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Models need access to fresh feature values for inference. Feature stores accomplish this by regularly recomputing features on an ongoing basis. Transformation jobs are orchestrated to ensure new data is processed and turned into fresh new feature values. These jobs are executed on data processing engines (e.g. Spark or Pandas) to which the feature store is connected. \r\n",
    "\r\n",
    "Model development introduces different transformation requirements. When iterating on a model, new features are often engineered to be used in training datasets that correspond to historical events (e.g. all purchases in the past 6 months). To support these use cases, feature stores make it easy to run “backfill jobs” that generate and persist historical values of a feature for training. Some feature stores automatically backfill newly registered features for preconfigured time ranges for registered training datasets.\r\n",
    "\r\n",
    "Transformation code is reused across environments preventing training-serving skew and frees teams from having to rewrite code from one environment to the next."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An example of a Feature Store containing the already mentioned three components can be seen here:\r\n",
    "\r\n",
    "<p align=center> <img src=images/Feature_Stores_TSS.png> </p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the image, Feature Management involves the Transformation part, and Access can be seen as Serving"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monitoring\r\n",
    "\r\n",
    "When something goes wrong in an ML system, it’s usually a data problem. Feature stores are uniquely positioned to detect and surface such issues. They can calculate metrics on the features they store and serve that describe correctness and quality. Feature stores monitor these metrics to provide a signal of the overall health of an ML application."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature data can be validated based on user defined schemas or other structural criteria. Data quality is tracked by monitoring for drift and training-serving skew. E.g. feature data served to models are compared to data on which the model was trained to detect inconsistencies that could degrade model performance.\r\n",
    "\r\n",
    "When running production systems, it’s also important to monitor operational metrics. Feature stores track operational metrics relating to core functionality. E.g. metrics relating to feature storage (availability, capacity, utilization, staleness) or feature serving (throughput, latency, error rates). Other metrics describe the operations of important adjacent system components. For example, operational metrics for external data processing engines (e.g. job success rate, throughput, processing lag and rate).\r\n",
    "\r\n",
    "Feature stores make these metrics available to existing monitoring infrastructure. This allows ML application health to be monitored and managed with existing observability tools in the production stack.\r\n",
    "\r\n",
    "Having visibility into which features are used by which models, feature stores can automatically aggregate alerts and health metrics into views relevant to specific users, models, or consumers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Registry\r\n",
    "\r\n",
    "A critical component in all feature stores is a centralized registry of standardized feature definitions and metadata. The registry acts as a single source of truth for information about a feature in an organization. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The registry is a central interface for user interactions with the feature store. Teams use the registry as a common catalog to explore, develop, collaborate on, and publish new definitions within and across teams.\r\n",
    "\r\n",
    "The definitions in the registry configure feature store system behavior. Automated jobs use the registry to schedule and configure data ingestion, transformation, and storage. It forms the basis of what data is stored in the feature store and how it is organized. Serving APIs use the registry for a consistent understanding of which feature values should be available, who should be able to access them, and how they should be served.\r\n",
    "\r\n",
    "The registry allows for important metadata to be attached to feature definitions. This provides a route for tracking ownership, project or domain specific information, and a path to easily integrate with adjacent systems. This includes information about dependencies and versions which is used for lineage tracking. \r\n",
    "\r\n",
    "To help with common debugging, compliance, and auditing workflows, the registry acts as an immutable record of what’s available analytically and what’s actually running in production.\r\n",
    "\r\n",
    "So far, we’ve looked at the core minimal components of a feature store. In practice, companies often have needs like compliance, governance, and security that require additional enterprise-focused capabilities. That will be the topic of a future blog post. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Store Providers\n",
    "\n",
    "Here we are enumerating a (non-exhaustive) list of commercial Feature Stores:\n",
    "\n",
    "- __Feast__: Feast has grown in popularity during the last months, and currently is one of the most used Feature Stores providers. It can be easily used with Python, and the library will handle many file management intricacies. This is a great option if you already created the transformation Pipeline\n",
    "- __Tecton__: Another very popular Feature Store provider is Tecton, which includes end-to-end pipelines for your features. This, as opposed to Feast, it supports transformations.\n",
    "- __AWS__: Amazon doesn't actually provide a stand-alone Feature Store service. Instead, it integrates this service in its SageMaker service.\n",
    "- __Hospworks__: It supports the full stack mentioned in this lesson, and it also includes a nice User Interface. Additionally, it can integrate more third-party services, so you can extend your pipeline easily.\n",
    "- __Iguazio__: This Feature Store has its own platform, and doesn't rely on other platforms to store the features. However, that doesn't mean that you can't use third-party services!\n",
    "\n",
    "You can get more information about each provider in this [link](https://mlops.community/learn/feature-store/), where you can easily compare them.\n",
    "\n",
    "During this module we are going to focus on Feast, since it is easy to integrate, it has a large community, and it is a great way to start with Feature Stores."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}